{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dfriedman/miniconda3/envs/nlp/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('hw1-sgd/data.csv').values\n",
    "np.random.shuffle(data)\n",
    "trainsize = int(len(data)*.8)\n",
    "train = data[:trainsize]\n",
    "test = data[trainsize:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Assignment Owner: Tian Wang\n",
    "\n",
    "#######################################\n",
    "#### Normalization\n",
    "\n",
    "\n",
    "def feature_normalization(train, test):\n",
    "    \"\"\"Rescale the data so that each feature in the training set is in\n",
    "    the interval [0,1], and apply the same transformations to the test\n",
    "    set, using the statistics computed on the training set.\n",
    "\n",
    "    Args:\n",
    "        train - training set, a 2D numpy array of size (num_instances, num_features)\n",
    "        test  - test set, a 2D numpy array of size (num_instances, num_features)\n",
    "    Returns:\n",
    "        train_normalized - training set after normalization\n",
    "        test_normalized  - test set after normalization\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    train_normalized = np.zeros(train.shape)\n",
    "    maxes, mins = [], []\n",
    "    \n",
    "    norm = lambda i,x: (x-mins[i])/(maxes[i]-mins[i])\n",
    "    \n",
    "    for i,row in enumerate(train):\n",
    "        maxes.append(row.max())\n",
    "        mins.append(row.min())\n",
    "        \n",
    "        train_normalized[i]= [norm(i,n) for n in row]\n",
    "    \n",
    "    test_normalized = np.zeros(test.shape)\n",
    "    for i,row in enumerate(test): \n",
    "        test_normalized[i] = [norm(i,n) for n in row]\n",
    "        \n",
    "    return train_normalized, test_normalized\n",
    "\n",
    "\n",
    "## test\n",
    "train_norm, test_norm = feature_normalization(train,test)\n",
    "def normed(num): assert num >= 0 and num <= 1\n",
    "[normed(n) for row in train_norm for n in row]\n",
    "assert train_norm.shape == train.shape\n",
    "assert test_norm.shape == test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118.2870746683847"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################################\n",
    "#### The square loss function\n",
    "\n",
    "def sq_loss(pred, actual):\n",
    "    return (pred-actual)**2\n",
    "\n",
    "def predict(x_i, theta):\n",
    "    return np.dot(x_i, theta)\n",
    "\n",
    "def compute_square_loss(X, y, theta):\n",
    "    \"\"\"\n",
    "    Given a set of X, y, theta, compute the square loss for predicting y with X*theta\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        theta - the parameter vector, 1D array of size (num_features)\n",
    "\n",
    "    Returns:\n",
    "        loss - the square loss, scalar\n",
    "    \"\"\"\n",
    "    loss = 0 #initialize the square_loss\n",
    "    for x_i, y_i in zip(X, y):\n",
    "        pred = predict(x_i, theta)\n",
    "        l = sq_loss(pred, y_i)\n",
    "        loss += l\n",
    "    \n",
    "    return loss/len(y)\n",
    "\n",
    "## test\n",
    "nfeatures = train_norm.shape[1]-1\n",
    "train_x = train_norm[:, :-1]\n",
    "train_y = train_norm[:, -1]\n",
    "theta = np.random.rand(nfeatures)\n",
    "\n",
    "compute_square_loss(train_x, train_y, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.23938745e+00, 3.56481121e+00, 1.21054780e+01, 9.22323488e+00,\n",
       "       8.98144245e+00, 4.60320515e+00, 1.39789426e+01, 3.91304186e+00,\n",
       "       7.47306091e+00, 1.46209181e+01, 7.49909691e+00, 9.94183301e+00,\n",
       "       2.03888907e+01, 1.36935562e+01, 1.82718903e+01, 2.45067880e+00,\n",
       "       9.68389515e+00, 5.84664683e+00, 8.88659186e+00, 9.04211512e+00,\n",
       "       3.33417207e+00, 3.77175191e+00, 1.19069530e+01, 3.68638739e+00,\n",
       "       1.38216540e+00, 5.17299599e+00, 8.24570608e+00, 1.77597255e+01,\n",
       "       1.28314587e+00, 7.50342823e+00, 1.16343274e-02, 1.08705953e+01,\n",
       "       1.51531741e+01, 1.90312281e+01, 2.69760350e+00, 1.28477910e+00,\n",
       "       1.66983188e+01, 5.76280011e+00, 1.08463549e+01, 1.42502659e+01,\n",
       "       1.37598320e+01, 1.02507931e+00, 1.18772437e+01, 1.10033288e+01,\n",
       "       1.62303292e+01, 3.36802869e+00, 1.70125115e+01, 1.88753163e+01])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################################\n",
    "### compute the gradient of square loss function\n",
    "def compute_square_loss_gradient(X, y, theta):\n",
    "    \"\"\"    \n",
    "    Compute gradient of the square loss (as defined in compute_square_loss), at the point theta.\n",
    "    \n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        theta - the parameter vector, 1D numpy array of size (num_features)\n",
    "\n",
    "    Returns:\n",
    "        grad - gradient vector, 1D numpy array of size (num_features)\n",
    "    \"\"\"\n",
    "    \n",
    "    grad = 0\n",
    "    for x_i, y_i in zip(X, y):\n",
    "        pred = predict(x_i, theta)\n",
    "        grad_i = np.dot(2*theta, (pred-y_i))\n",
    "        grad += grad_i\n",
    "    \n",
    "    return np.array(grad/len(y))\n",
    "    \n",
    "compute_square_loss_gradient(train_x, train_y, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "### Gradient Checker\n",
    "#Getting the gradient calculation correct is often the trickiest part\n",
    "#of any gradient-based optimization algorithm.  Fortunately, it's very\n",
    "#easy to check that the gradient calculation is correct using the\n",
    "#definition of gradient.\n",
    "#See http://ufldl.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization\n",
    "def grad_checker(X, y, theta, epsilon=0.01, tolerance=1e-4):\n",
    "    \"\"\"Implement Gradient Checker\n",
    "    Check that the function compute_square_loss_gradient returns the\n",
    "    correct gradient for the given X, y, and theta.\n",
    "\n",
    "    Let d be the number of features. Here we numerically estimate the\n",
    "    gradient by approximating the directional derivative in each of\n",
    "    the d coordinate directions:\n",
    "    (e_1 = (1,0,0,...,0), e_2 = (0,1,0,...,0), ..., e_d = (0,...,0,1)\n",
    "\n",
    "    The approximation for the directional derivative of J at the point\n",
    "    theta in the direction e_i is given by:\n",
    "    ( J(theta + epsilon * e_i) - J(theta - epsilon * e_i) ) / (2*epsilon).\n",
    "\n",
    "    We then look at the Euclidean distance between the gradient\n",
    "    computed using this approximation and the gradient computed by\n",
    "    compute_square_loss_gradient(X, y, theta).  If the Euclidean\n",
    "    distance exceeds tolerance, we say the gradient is incorrect.\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        theta - the parameter vector, 1D numpy array of size (num_features)\n",
    "        epsilon - the epsilon used in approximation\n",
    "        tolerance - the tolerance error\n",
    "\n",
    "    Return:\n",
    "        A boolean value indicate whether the gradient is correct or not\n",
    "\n",
    "    \"\"\"\n",
    "    true_gradient = compute_square_loss_gradient(X, y, theta) #the true gradient\n",
    "    num_features = theta.shape[0]\n",
    "    approx_grad = np.zeros(num_features) #Initialize the gradient we approximate\n",
    "    #TODO\n",
    "\n",
    "#################################################\n",
    "### Generic Gradient Checker\n",
    "def generic_gradient_checker(X, y, theta, objective_func, gradient_func, epsilon=0.01, tolerance=1e-4):\n",
    "    \"\"\"\n",
    "    The functions takes objective_func and gradient_func as parameters. And check whether gradient_func(X, y, theta) returned\n",
    "    the true gradient for objective_func(X, y, theta).\n",
    "    Eg: In LSR, the objective_func = compute_square_loss, and gradient_func = compute_square_loss_gradient\n",
    "    \"\"\"\n",
    "    #TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "#### Batch Gradient Descent\n",
    "def batch_grad_descent(X, y, alpha=0.1, num_iter=1000, check_gradient=False):\n",
    "    \"\"\"\n",
    "    In this question you will implement batch gradient descent to\n",
    "    minimize the square loss objective\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        alpha - step size in gradient descent\n",
    "        num_iter - number of iterations to run\n",
    "        check_gradient - a boolean value indicating whether checking the gradient when updating\n",
    "\n",
    "    Returns:\n",
    "        theta_hist - store the the history of parameter vector in iteration, 2D numpy array of size (num_iter+1, num_features)\n",
    "                    for instance, theta in iteration 0 should be theta_hist[0], theta in ieration (num_iter) is theta_hist[-1]\n",
    "        loss_hist - the history of objective function vector, 1D numpy array of size (num_iter+1)\n",
    "    \"\"\"\n",
    "    num_instances, num_features = X.shape[0], X.shape[1]\n",
    "    theta_hist = np.zeros((num_iter+1, num_features))  #Initialize theta_hist\n",
    "    loss_hist = np.zeros(num_iter+1) #initialize loss_hist\n",
    "    theta = np.random.rand(num_features) #initialize theta\n",
    "    for epoch in range(num_iter):\n",
    "        grads = compute_square_loss_gradient(X, y, theta)\n",
    "        loss = compute_square_loss(X, y, theta)\n",
    "        \n",
    "        if(loss == np.inf):\n",
    "            print(f'inf reached at epoch {epoch}')\n",
    "            break\n",
    "          \n",
    "        theta_hist.put(epoch, theta)\n",
    "        loss_hist.put(epoch, loss)\n",
    "        theta = theta - alpha * grads\n",
    "    \n",
    "    return theta_hist, loss_hist\n",
    "\n",
    "####################################\n",
    "###Q2.4b: Implement backtracking line search in batch_gradient_descent\n",
    "###Check http://en.wikipedia.org/wiki/Backtracking_line_search for details\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "### Compute the gradient of Regularized Batch Gradient Descent\n",
    "def compute_regularized_square_loss_gradient(X, y, theta, lambda_reg):\n",
    "    \"\"\"\n",
    "    Compute the gradient of L2-regularized square loss function given X, y and theta\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        theta - the parameter vector, 1D numpy array of size (num_features)\n",
    "        lambda_reg - the regularization coefficient\n",
    "\n",
    "    Returns:\n",
    "        grad - gradient vector, 1D numpy array of size (num_features)\n",
    "    \"\"\"\n",
    "    \n",
    "    grad = 0\n",
    "    for x_i, y_i in zip(X, y):\n",
    "        pred = predict(x_i, theta)\n",
    "\n",
    "        grad_i = np.dot(2*theta, (pred-y_i)) + lambda_reg * theta.T\n",
    "        grad += grad_i\n",
    "    \n",
    "    return grad/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "### Batch Gradient Descent with regularization term\n",
    "def regularized_grad_descent(X, y, alpha=0.1, lambda_reg=1, num_iter=1000):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        alpha - step size in gradient descent\n",
    "        lambda_reg - the regularization coefficient\n",
    "        numIter - number of iterations to run\n",
    "\n",
    "    Returns:\n",
    "        theta_hist - the history of parameter vector, 2D numpy array of size (num_iter+1, num_features)\n",
    "        loss_hist - the history of loss function without the regularization term, 1D numpy array.\n",
    "    \"\"\"\n",
    "    (num_instances, num_features) = X.shape\n",
    "    theta = np.zeros(num_features) #Initialize theta\n",
    "    theta_hist = np.zeros((num_iter+1, num_features))  #Initialize theta_hist\n",
    "    loss_hist = np.zeros(num_iter+1) #Initialize loss_hist\n",
    "\n",
    "    num_instances, num_features = X.shape[0], X.shape[1]\n",
    "    theta_hist = np.zeros((num_iter+1, num_features))  #Initialize theta_hist\n",
    "    loss_hist = np.zeros(num_iter+1) #initialize loss_hist\n",
    "    theta = np.random.rand(num_features) #initialize theta\n",
    "    for epoch in range(num_iter):\n",
    "        grads = compute_regularized_square_loss_gradient(X, y, theta, lambda_reg)\n",
    "        loss = compute_square_loss(X, y, theta)\n",
    "        \n",
    "        if(loss == np.inf):\n",
    "            print(f'inf reached at epoch {epoch}')\n",
    "            break\n",
    "          \n",
    "        theta_hist.put(epoch, theta)\n",
    "        loss_hist.put(epoch, loss)\n",
    "        theta = theta - alpha * grads\n",
    "    \n",
    "    return theta_hist, loss_hist\n",
    "\n",
    "#############################################\n",
    "## Visualization of Regularized Batch Gradient Descent\n",
    "##X-axis: log(lambda_reg)\n",
    "##Y-axis: square_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "#############################################\n",
    "### Stochastic Gradient Descent\n",
    "def stochastic_grad_descent(X, y, alpha=0.1, lambda_reg=1, num_iter=1000):\n",
    "    \"\"\"\n",
    "    In this question you will implement stochastic gradient descent with a regularization term\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        alpha - string or float. step size in gradient descent\n",
    "                NOTE: In SGD, it's not always a good idea to use a fixed step size. Usually it's set to 1/sqrt(t) or 1/t\n",
    "                if alpha is a float, then the step size in every iteration is alpha.\n",
    "                if alpha == \"1/sqrt(t)\", alpha = 1/sqrt(t)\n",
    "                if alpha == \"1/t\", alpha = 1/t\n",
    "        lambda_reg - the regularization coefficient\n",
    "        num_iter - number of epochs (i.e number of times) to go through the whole training set\n",
    "\n",
    "    Returns:\n",
    "        theta_hist - the history of parameter vector, 3D numpy array of size (num_iter, num_instances, num_features)\n",
    "        loss hist - the history of regularized loss function vector, 2D numpy array of size(num_iter, num_instances)\n",
    "    \"\"\"\n",
    "    \n",
    "    num_instances, num_features = X.shape[0], X.shape[1]\n",
    "    theta = np.random.rand(num_features) #Initialize theta\n",
    "\n",
    "    theta_hist = np.zeros((num_iter, num_instances, num_features))  #Initialize theta_hist\n",
    "    loss_hist = np.zeros((num_iter, num_instances)) #Initialize loss_hist\n",
    "    \n",
    "    for epoch in range(num_iter):\n",
    "        data = [d for d in zip(X,y)]\n",
    "        np.random.shuffle(data)\n",
    "        \n",
    "        epoch_losses = np.zeros(num_instances)\n",
    "        epoch_thetas = np.zeros(num_instances)\n",
    "        \n",
    "        for ix, (x_i,y_i) in enumerate(data):\n",
    "            pred = predict(x_i, theta)\n",
    "            grads = np.dot(2*theta, (pred-y_i)) + lambda_reg * theta.T\n",
    "            theta = theta - alpha * grads\n",
    "            \n",
    "            loss = sq_loss(predict(x_i, theta), y_i)\n",
    "            if(loss == np.inf):\n",
    "                print(f'inf reached at epoch {epoch}')\n",
    "                break\n",
    "            np.put(epoch_losses, ix, loss)\n",
    "            np.put(epoch_thetas, ix, theta)\n",
    "        np.put(theta_hist, epoch, epoch_thetas)\n",
    "        np.put(loss_hist, epoch, epoch_losses)\n",
    "    \n",
    "    return theta_hist, loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the dataset\n",
      "Split into Train and Test\n",
      "Scaling all to [0, 1]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main():\n",
    "    #Loading the dataset\n",
    "    print('loading the dataset')\n",
    "\n",
    "    df = pd.read_csv('hw1-sgd/data.csv', delimiter=',')\n",
    "    X = df.values[:,:-1]\n",
    "    y = df.values[:,-1]\n",
    "\n",
    "    print('Split into Train and Test')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=100, random_state=10)\n",
    "\n",
    "    print(\"Scaling all to [0, 1]\")\n",
    "    X_train, X_test = feature_normalization(X_train, X_test)\n",
    "    X_train = np.hstack((X_train, np.ones((X_train.shape[0], 1))))  # Add bias term\n",
    "    X_test = np.hstack((X_test, np.ones((X_test.shape[0], 1)))) # Add bias term\n",
    "\n",
    "    th, batch_loss = batch_grad_descent(X_train, y_train, 0.01, 50)\n",
    "    th, reg_loss = regularized_grad_descent(X_train, y_train, 0.01, 15, 50)\n",
    "    th, sgd_loss = stochastic_grad_descent(X_train, y_train, 0.01, 1, 50)\n",
    "    \n",
    "    plt.plot(range(len(batch_loss)), batch_loss, label='Batch')\n",
    "    plt.plot(range(len(reg_loss)), reg_loss, label='Regularized')\n",
    "    plt.plot(range(sgd_loss.shape[0]), [np.average(l) for l in sgd_loss], label='SGD')\n",
    "    \n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('sq loss')\n",
    "    plt.title(\"Grad Descent\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
